---
title: STAT 506 - Advanced Regression Analysis
framework: bootplus
layout: poststripped
mode: selfcontained
highlighter: prettify
hitheme    : twitter-bootstrap
widgets    : [mathjax]
navbar:
  title: Kevin Ferris | Final Exam
lead : >
  April 30, 2014
assets:
  css:
  - "http://fonts.googleapis.com/css?family=Raleway:300"
  - "http://fonts.googleapis.com/css?family=Oxygen"
  jshead:
  - "./lodash.js"
  - "./dygraph-combined.js"
--- 
### Number 1 Part A

```{r echo=F, warning=F, error=F, message=F, cache = F}
# setwd("~/School/Stat 506 - Advanced Regression/Final/final506_writeup")
require(knitr)
opts_chunk$set(
  echo=FALSE,
  message=F,
  cache = F,
  fig.width = 8,
  fig.height = 6, 
  fig.align = 'center', 
  size = 'small'
)
```

```{r rsetup}
require(ggplot2)
require(gridExtra)
require(lme4)
require(knitr)
require(nlme)
require(R2jags)
require(car)
require(plyr)
require(reshape2)
require(mvrnorm)
source("panelStackedDens.R")
set.seed(42)
```

```{r data}
setwd("~/School/Stat 506 - Advanced Regression/Final")
math1 <- read.csv("mathComparison.csv", header = T)

##### centering #############
math1$pre.cent <- math1$mathPrescore - mean(math1$mathPrescore)
math1$ses.cent <- math1$ses - mean(math1$ses)
math1$prep.cent <- math1$teachrMathprep - mean(math1$teachrMathprep)
math1$math.cent <- math1$teachrMath - mean(math1$teachrMath, na.rm = T)
math1$xper.cent <- math1$teachXper - mean(math1$teachXper)
math1$pov.cent <- math1$povertyRate - mean(math1$povertyRate)

##### dealing with missingness ###############
math1$missing <- factor(ifelse(is.na(math1$teachrMath), "miss", "not"))

math2 <- read.csv("mathImprovement.csv", header = T)
math3 <- melt(math2, id.vars = "id", value.name = "Score", variable.name = "Grade")
math3$Grade <- as.numeric(gsub("grd", "", as.character(math3$Grade)))
math3$gr.cent <- math3$Grade - mean(math3$Grade)
source("~/R Stuff/vwReg_v4.R")
```

```{r loadOutput}
setwd("~/School/Stat 506 - Advanced Regression/Final/final506_writeup")
load("num2_out.RData")
load("num1b.RData")
load("num1c_out.RData")
```


Poverty rate is measured at the school level.

The number of classes the teacher has taken, the teacher's mathematical knowledge, and the number of years of experience of the teacher are measured at the class level.

SES, math score in kindergarten, sex, and minority status are measured on the individual level.

---
### Number 1 Part B

The first thing I did with these data was look at the missingness patterns because some of the scores of teacher's mathematics knowledge were missing.  On the left is the distribution of the increase in math score for both the missing and non-missing data.  The two distributions look to be fairly similar; the only noticeable difference is that all the students whose math scores decreased by 100 are from the missing data.  
```{r missingPlots, fig.width=10, fig.height=5}
p1 <- ggplot(math1, aes(x=teachrMathprep, y = ..density..)) +
  geom_histogram(binwidth = .5, colour = "darkgreen", fill = "grey") + 
  facet_wrap(~missing) + theme_bw() + 
  xlab("Number of Math Classes the Teacher has Taken")
p2 <- ggplot(math1, aes(x=mathGain, y = ..density..)) +
  geom_histogram(colour = "darkgreen", fill = "grey") + 
  facet_wrap(~missing) + theme_bw() + 
  xlab("Math Gain")
grid.arrange(p2, p1, ncol = 2)

# removing missing data
math1 <- na.omit(math1)
math1$schoolid <- as.integer(as.factor(math1$schoolid))
math1$classid <- as.integer(as.factor(math1$classid))
```
For most of the explanatory variables, there was not much of a difference in the distribution between the missing and non-missing data.  However, on the right in the plot above it looks like the distribution of teacher's math preparation is plotted, and it looks like slightly more inexperienced teacher's are likely to have missing data.  The same pattern held for poverty rate --- poorer schools were slightly more likely to be missing.  This means that we will need to be very careful about the inferences we make for these data because it is not safe to assume that the data are missing at random. Even though we have random selection, we probably shouldn't infer the results beyond these data nor should we make causal inferences.

Because we are mostly interested in exploring the relationships between the variables and not in finding a best fitting model, I decided to use Gelman's model selection strategy: keep all variables in the model unless they have a sign we don't expect and are not significant.  With that in mind, I should lay out the expected sign for each variable:

```{r expectTab, results='asis'}
expect.tab <- data.frame(
  Variable = c("sex", "minortiy", "SES", "mathPrescore", "teachrXper", "teachrMath", "teacherMathprep", "povertyRate"), 
  Level = c(rep("individual", 4), rep("class", 3), "school"), 
  Expected.Sign = c("positive for female", "negative", "positive", "positive", "positive", "positive", "positive", "negative"))
kable(expect.tab)
```

Next, I wanted to consider what interactions would be allowed in the model.  I decided to only consider interactions of categorical and quantitative variables at the individual level.  I then made plots like the one below to assess interactions.  To make this plot, 1000 bootstrap replicates of the data were made.  A loess smoother was then fit to each replicate.  Then, the density of the fitted values at 200 equal spaced intervals of the mathprescore variable was calculated.  The intensity of the color is proportional to the density.  So, in areas where the color is more intense, we are more certain of the location of the true mean of math gain.
```{r vwplot, eval=FALSE}
p4 <- vwReg(mathGain ~ mathPrescore, data = subset(math1, sex == "M"), 
            spag = TRUE, shade = FALSE)
p5 <- vwReg(mathGain ~ mathPrescore, data = subset(math1, sex == "F"), 
            spag = TRUE, shade = FALSE, add = TRUE, spag.color="red", shape=3)
p6 <- p4 + p5 + 
  ggtitle("Relationship between Math Gain and pre Score by Gender") + 
  geom_text(aes(x = 350, y = -50), label = "Red is for men \n Blue is for Women")
```
<iframe width = "723" height = "406" src = "vwSex_prescore.png"></iframe>

In this plot, it really appears that there is no difference in the relationship between men and women so I didn't include an interaction between pre-score and sex in the model.  There are a couple of notable features in the plot though.  It looks as though pre-score is truncated at 300 and 650.  The relationship looks to be fairly linear everywhere else, but, because of this truncation, things may not be entirely linear in the tails.  We'll have to check the residuals to make sure that things are linear.

The plot below looks for an interaction between pre-score and minority status.  Here, there does appear to be an interaction present.  It seems that the slope is more negative for minorities than for non-minorities so I will include an interaction term for minority and pre-score in the model.
```{r vwplot2, eval=FALSE}
p7 <- vwReg(mathGain ~ mathPrescore, data = subset(math1, minority == 0), 
            spag = TRUE, shade = FALSE)
p8 <- vwReg(mathGain ~ mathPrescore, data = subset(math1, minority == 1), 
            spag = TRUE, shade = FALSE, add = TRUE, spag.color="red", shape=3)
p9 <- p7 + p8 + 
  ggtitle("Relationship between Math Gain and Pre-Score by Minority Status") + 
  geom_text(aes(x = 350, y = -50), label = "Red is for minorities \n Blue is for non-minorities")
```
<iframe width = "723" height = "406" src = "vwMinority_prescore.png"></iframe>
Neither minority status nor sex seemed to have a strong interaction with SES when I made these plots, so I did not include those interactions.

I included random intercepts for both school and class-within-school only.  I considered random intercepts, but they did not seem to provide any improvement to the model (measured using AICs) so I decided not to include any of them.  The final model I used is:

$Gain_i = \beta_0 + \beta_1 * SES_i + \beta_2 * pre_i + \beta_3 * I(minority_i == 1) + \beta_4 * I(sex_i == M) + \\ \beta_5 * I(minority_i == 1) * pre_i + \beta_6 * teacherPrep_{j[i]} + \\ \beta_7 * teacherExper_{j[i]} + \beta_8 * teacherMath_{j[i]} + \beta_9 * povertyRate_{k[i]} + \\ 
b_{1,j[i]} + b_{2,k[i]} +  \epsilon_i$

In this model $i$ denotes the student, $j[i]$ denotes the class for the $i^{th}$ student, and $k[i]$ denotes the school for the $i^{th}$ student.  $b_2$ is a random intercept for the $k^{th}$ school, $b_1$ is a random effect for the $j^{th}$ class in the $k^{th}$ school, and $\epsilon_i$ is a random error term for the $i^{th}$ student.  I centered all quantitative explanatory variables in this model.

```{r num1blmer, eval=FALSE}
fit0 <- lmer(mathGain ~ (1|schoolid) + (1|classid) + 
               ses.cent + pre.cent*minority + sex + 
               prep.cent + xper.cent + math.cent + pov.cent, 
             data = math1)
ints <- confint(fit0, level = c(0,.5, .95))
```


Caterpillar plots for the fixed effects from the model are provided below.  These variables are all on different scales (in hindsight, I probably should have standardized everything) so I won't interpret the magnitude of the effect; instead, I'll just report on which variables have a positive or negative effect.  

```{r catPlots, fig.width=12, fig.height=6}
# this code is really sloppy.  I would recommend trying to read it
# But it works!
fix.ints <- ints[-(1:4), -1]
s1 <- ggplot() +  
  geom_segment(aes(x = 1:6, xend = 1:6, y = fix.ints[-c(3:4,9), 2], yend = fix.ints[-c(3:4,9), 5]), 
               colour = "purple") + 
  geom_segment(aes(x = 1:6, xend = 1:6, y = fix.ints[-c(3:4,9), 1], yend = fix.ints[-c(3:4,9), 4]), 
               size = I(1.2), colour = "red") + 
  geom_point(aes(x = 1:6, y = fix.ints[-c(3:4,9), 3]), size = I(3)) + 
  geom_hline(aes(yintercept = 0), linetype = "dashed", size = I(0.5)) + 
  scale_x_continuous(breaks = 1:6, 
                     labels = c("SES", "PreScore", 
                                "TeachPrep", "TeachXper", "TeachMath", 
                                "Poverty")) + 
  labs(x = "Parameter", y = "Estimate", 
       title = "Slopes") + theme_bw() + coord_flip()
s2 <- ggplot() +  
  geom_segment(aes(x = 1:2, xend = 1:2, y = fix.ints[(3:4), 2], yend = fix.ints[(3:4), 5]), 
               colour = "purple") + 
  geom_segment(aes(x = 1:2, xend = 1:2, y = fix.ints[(3:4), 1], yend = fix.ints[(3:4), 4]), 
               size = I(1.2), colour = "red") + 
  geom_point(aes(x = 1:2, y = fix.ints[(3:4), 3]), size = I(3)) + 
  geom_hline(aes(yintercept = 0), linetype = "dashed", size = I(0.5)) + 
  scale_x_continuous(breaks = 1:2, 
                     labels = c("Minority", "Male")) + 
  labs(x = "Parameter", y = "Estimate", 
       title = "Intercept Adjustments") + theme_bw() + coord_flip()
grid.arrange(s1, s2, ncol = 2)
```

It appears as though there is not much evidence to suggest that sex, teacher's experience, teacher's math knowledge, or school's poverty rate,  have a strong positive or negative effect on student's math improvement.  For each of these terms, there is just too much uncertainty to say that they have a positive or negative effect on math improvement.  There may be some evidence that teacher's math background has a positive effect on student's improvement.

SES does tend to have a positive effect on student's math improvements.  Interestingly, the pre-score for both minorities and non-minorities has a negative relationship with math gain.  This could mean that student's who are already good don't tend to get much better.  Alternatively, it could me that we are seeing a strong regression to the mean in these data.  This would suggest that student's who did very well in kindergarten were just lucky and that luck dissipated the next year.

There is extremely strong evidence that the intercept adjustment for minorities is pretty negative.  This suggests that, on average, minorities tend to have smaller improvements than non-minorities.  I did not include a caterpillar interval for the interaction term because it is fairly small.  But there is some evidence that the slope on pre-score is more negative for minorities; this would suggest that minority students who were initially strong did not improve as much as non-minorities who were initially strong.

The variances of both the class and school effect are both one-tenth the variance of the residual error.  This suggests that, relative to the noise is the data, the variability between schools and between classes within a school is not very large.

As we noted earlier, we should look at the pattern of the residuals vs pre-score to make sure that there are no violations of linearity.  The plot is located below, and there do not appear to be any patterns so this assumption is probably okay.
```{r residVsPre, fig.width=6, fig.height=4}
qplot(math1$pre.cent, resid(fit0)) + theme_bw() + 
  labs(x = "Pre-Score", y = "Residuals")
```

The residuals vs fitted values plot is located below.  There do not appear to be any noticeable patterns so the constant variance assumption is probably okay.
```{r residVsFitted, fig.width=6, fig.height=4}
plot(fit0)
```

In the model, I assumed that all the random terms were normally distributed.  I can check these assumptions by making normal quantiles plots for each term.  These plots are located below.  There are a few outlying points in the tail of the residuals, but we have a large data set so this is not too uncommon.  Otherwise, these plots all look pretty good so the normality assumptions are probably okay.
```{r normalQQ, fig.width=12, fig.height=4}
par(mfrow = c(1,3))
qqPlot(resid(fit0), main = "Residuals")
qqPlot(ranef(fit0)$classid, main = "Class Effects")
qqPlot(ranef(fit0)$schoolid, main = "School Effects")
```


To conclude, it looks as though the only strong predictors of math improvement are SES, pre-score, and minority status.  For the other variables, there is just too much uncertainty or the effect is too small to say that there is much of a relationship.  As noted before, we should be careful with our inferences for these data.  These results only hold for the 1081 students for whom we don't have missing data, and we can't say that any of the predictors cause the increase/decrease in the math improvement that we observed.

---
### Number 1 Part C

I fit the same model in JAGS using non-informative parameters.  I ran four chains, discarding the first 2000 iterations.  I then used a thinning interval of 8 for the next 2000 iterations.  This resulted in 1000 draws of the parameters.  Convergence diagnostics show that the posterior distributions had converged to stationary distributions.  Gelman's $\hat{R}$ was less than 1.02 for all the parameters and all the parameters had effective sample sizes of at least 150.  Trace plots of the parameters are provided below, and they show that the chains are mixing well.

```{r jagsTracePlot, fig.height=10, fig.width=15}
plot(out2, density = FALSE)
```


A summary of the parameters is provided in the table below.  These are now credible intervals obtained from the posterior distribution of each parameter.  This means that they have a slightly different interpretation.  For example, if we wanted to interpret the coefficient on SES, we would say that there is a 95% chance that a 1 unit increase in SES is associated with between a 2.72 and 7.79 true mean gain in math score after accounting for all other variables in the model.

```{r num1cSummary, results='asis'}
mytab <- summary(out2)$quantiles[2:13, ]
rownames(mytab) <- c("preScore", "SES", "minority", "male", "Interaction", 
                  "TeachPrep", "TeachXper", "TeachMath", 
                  "Poverty", 
                  "School SD", "Class SD", "Residual SD")
kable(mytab)
```



---
### Number 2

For this problem, we are given information of the math scores of 500 students in grades 6 through 9.  The distribution of score in each grade is plotted below.  It looks as though there is a linear relationship between score and grade so I treated grade as a quantitative variable.

```{r scoreBoxPlots}
ggplot(math3, aes(factor(Grade), Score)) + 
  geom_boxplot(colour = "darkgreen", fill = "grey") + 
  theme_bw() + 
  labs(x = "Grade")
```

The trajectory of each child over time is plotted below.  There appears to be a lot of variability in the intercepts of each child.  Though there is not as much variability in the slopes, it still looks like it might be worth allowing the slopes to vary by student.

```{r scoreLinePlot}
ggplot(math3, aes(Grade, Score, group = id)) + 
  geom_line(alpha = I(.1)) + theme_bw()
```

The model used was $y_i = \beta_0 + \beta_1 * grade_i + \beta_{2,j[i]} + \beta_{3,j[i]} * grade_i + \epsilon_i$.  Here, $y_i$ is the math score for the $i^{th}$ observation.  $\beta_{2,j[i]}$ and $\beta_{3,j[i]}$ follow a multivariate normal distribution with a mean vector of 0 and a 2x2 variance-covariance matrix.  This matrix has $\sigma_{\beta2}^2$ and $\sigma_{\beta3}^2$ on the diagonal and $\sigma_{\beta2} * \sigma_{\beta3} * \rho$ on the off-diagonal.  The errors are assumed to be independent of the random effects and to be $\sim N(0, \sigma_y^2)$.

I put diffuse normal priors on the fixed effects in the model.  I used a scaled-inverse Wishart distribution with 3 df as the prior on the random slope, random intercept, and the correlation between the two.  I used a diffuse uniform prior on the residual variability in the data ($\sigma_y^2$).

```{r jagsMod, eval=FALSE}

cat("model {
  for(i in 1:N) {
    y[i] ~ dnorm(yhat[i], tau.y)
    yhat[i] <- b0 + b1 * gr[i] + 
      a0[id[i]] + a1[id[i]] * gr[i]
  }  

  b0 ~ dnorm(0, .001)
  b1 ~ dnorm(0, .001)
  tau.y <- pow(sig.y, -2)
  sig.y ~ dunif(0, 1000)

  for(j in 1:J) {
    a0[j] <- B[j,1]
    a1[j] <- B[j,2]
    B[j,1:2] ~ dmnorm(B.hat[j,], Tau.B.raw[,])
    B.hat[j,1] <- 0
    B.hat[j,2] <- 0
  }

  Tau.B.raw[1:2,1:2] ~ dwish(W[,], df)
  df <- 3
  Sigma.B.raw[1:2,1:2] <- inverse(Tau.B.raw[, ])
  sig.a0 <- sqrt(Sigma.B.raw[1,1])
  sig.a1 <- sqrt(Sigma.B.raw[2,2])
  rho <- Sigma.B.raw[1,2] / sqrt(Sigma.B.raw[2,2] * Sigma.B.raw[1,1])

    }", file = "~/School/Stat 506 - Advanced Regression/Final/Miscellaneous/num2.txt")

jags2.dat <- with(math3, list(
  N = nrow(math3), gr = gr.cent, y = Score, J = length(unique(id)), id = id, W = diag(2)))
parms2 <- c("b0", "b1", "sig.y", "sig.a0", "sig.a1", "rho")
fits.ret <- c("yhat", "sig.y")
inits <- function() {
  list(b0 = rnorm(1, 2, .05), 
       b1 = rnorm(1, 1, .05), 
       sig.y = rnorm(1, .7, .1))
}

pre2 <- jags.model("~/School/Stat 506 - Advanced Regression/Final/Miscellaneous/num2.txt", 
                   jags2.dat, n.adapt = 1000, n.chains = 4, inits = inits())
out2 <- coda.samples(pre2, parms2, n.iter=1000, thin=4) 
save(out2, file = "num2_out.RData")
```

4 chains were used for the MCMC sampling, and the first 1000 iterations were discarded as a warm-up.  A thinning interval of 4 was used for the next 1000 observations.  The diagnostics for the model look pretty good.  Gelman's $\hat{R}$ are less than 1.02 for all the terms and the multivariate $\hat{R}$ is 1.02.  Traceplots for each term are provided below and show that the chains appear to be mixing well.  Overall, it looks like things have converged.

```{r num2trace, fig.width=12, fig.height=8}
par(mfrow = c(3,2))
plot(out3, density = FALSE)
```

Summaries of the posterior distribution for each of the terms in the model are provided below.  It does appear that math scores do tend to increase over time.  Based on this model and data, there is a 95% chance that moving from one grade to the next is associated with between a 0.97 and 1.07 increase in the true mean math score.  

```{r num2summ, results='asis'}
summ.quant <- summary(out3)$quantiles
rownames(summ.quant) <- c("Intercept", "Slope", "Correlation", "Intercept SD", 
                          "Slope SD", "Residual SD")
kable(summ.quant, digits = 3)
```

It is easier to plot these results.  Below is a plot of the posterior distribution of the true mean math score for each grade on the left and the posterior distribution for a new student on the right.  We can see that there is slightly more variability in the scores for a new student, as we'd expect.  There is not much more though.  This suggests that there is not much residual variability - most of the uncertainty in these data concerns the location of the mean score for each grade.

There is also more uncertainty as students get older.  I theorize that this is because there's just not that much of a difference between good students and weaker students at the younger ages.  As the students age, the good students advance into geometry and trig while the weaker students fall behind a little bit.  The increased variability for older students originates because we start to see students differentiate themselves by ability.
```{r num2post, fig.width=12, fig.height=5}
draws <- ldply(out3, data.frame)
mean.draw <- t(apply(draws, 1, function(x) {
  vcov.mat <- matrix(c(x[4]^2, rep(prod(x[3:5]), 2), x[5]^2), ncol = 2)
  rndm.eff <- mvrnorm(1, c(0,0), vcov.mat)
  c(x[1] + x[2] * (6:9 - 7.5) + rndm.eff[1] + rndm.eff[2] * (6:9 - 7.5), x[6])
}))
mean.melt <- melt(mean.draw[, 1:4])
names(mean.melt) <- c("Simulation", "Grade", "Score")
mean.melt$Grade <- factor(rep(6:9, each = 1000))
new.draw <- t(apply(mean.draw, 1, function(x) rnorm(4, x[1:4], x[5])))
new.melt <- melt(new.draw)
names(new.melt) <- c("Simulation", "Grade", "Score")
new.melt$Grade <- factor(new.melt$Grade + 5)

overlap <- 0.5
h2 <- bwplot(Score ~ Grade, data = mean.melt, 
       panel = panel.stackedDens, overlap = overlap, xlab = "Grade", 
       lattice.options = list(axis.padding = list(factor = c(0.6, 0.7 + overlap))))
h1 <- bwplot(Score ~ Grade, data = new.melt, 
       panel = panel.stackedDens, overlap = overlap, xlab = "Grade", 
       lattice.options = list(axis.padding = list(factor = c(0.6, 0.7 + overlap))))
grid.arrange(h2, h1, ncol = 2)
```

Since these students are randomly selected, we can say that these improvements that we've found in math scores will hold true for all students.  We can't make causal inferences, but we can say that as students increase in age by one year, their true mean math achievement score tends to increase by 1 (with a 95% credible interval from 0.975 to 1.078) 

---
### R Code












